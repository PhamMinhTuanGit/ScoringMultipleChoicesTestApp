{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "collapsed_sections": [
        "QjCVbhrqK40D"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PhamMinhTuanGit/ScoringMultipleChoicesTestApp/blob/main/mamba.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mamba Inference Script\n",
        "by [Trelis.com](https://Trelis.com).\n",
        "\n",
        "Find us on [YouTube](https://Youtube.com/@TrelisResarch).\n",
        "\n",
        "Built from: [State spaces, Mamba on GitHub](https://github.com/state-spaces/mamba/blob/main/benchmarks/benchmark_generation_mamba_simple.py)\n",
        "\n",
        "## Getting started\n",
        "Select a GPU from Runtime -> Change Runtime type."
      ],
      "metadata": {
        "id": "S8UDlHsa1ilJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZCTReYFZz6LN"
      },
      "outputs": [],
      "source": [
        "!pip install causal-conv1d>=1.1.0 -q -U"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install mamba-ssm -q -U"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GDBxFNcxz-Dc",
        "outputId": "4fb1dd2c-9b4b-49d4-9e5a-1c59785e8213"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/91.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m91.8/91.8 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for mamba-ssm (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/state-spaces/mamba.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YhfaPaHSz_7Z",
        "outputId": "bab7b28e-d347-482a-80d1-680efd719650"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'mamba'...\n",
            "remote: Enumerating objects: 715, done.\u001b[K\n",
            "remote: Counting objects: 100% (32/32), done.\u001b[K\n",
            "remote: Compressing objects: 100% (17/17), done.\u001b[K\n",
            "remote: Total 715 (delta 23), reused 15 (delta 15), pack-reused 683 (from 2)\u001b[K\n",
            "Receiving objects: 100% (715/715), 1.55 MiB | 11.85 MiB/s, done.\n",
            "Resolving deltas: 100% (383/383), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd mamba"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q2FeujCy0Ry5",
        "outputId": "f4f6cab4-0d37-4f7a-e3e9-2d2c0ecfbc2c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/mamba\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!echo \"/usr/lib64-nvidia/\" | sudo tee -a /etc/ld.so.conf.d/libcuda.conf\n",
        "!ldconfig"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aFps7u-D4fH1",
        "outputId": "267db5a1-563f-47bf-e289-69e4d2586d54"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/lib64-nvidia/\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libhwloc.so.15 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm_debug.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libumf.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm.so.1 is not a symbolic link\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## One line example"
      ],
      "metadata": {
        "id": "QjCVbhrqK40D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python benchmarks/benchmark_generation_mamba_simple.py --model-name \"Trelis/mamba-2.8b-slimpj-bf16\" --prompt \"Here is a python program to add the first five fibonnacci numbers:\\n\" --topp 0.9 --temperature 0.7 --repetition-penalty 1.2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EaeESrKK0Ec4",
        "outputId": "6fa9f831-faf1-447d-ba5d-be56bb43be5c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading model Trelis/mamba-2.8b-slimpj-bf16\n",
            "config.json: 100% 184/184 [00:00<00:00, 1.50MB/s]\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/mamba/benchmarks/benchmark_generation_mamba_simple.py\", line 40, in <module>\n",
            "    tokenizer = AutoTokenizer.from_pretrained(args.model_name)\n",
            "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/models/auto/tokenization_auto.py\", line 992, in from_pretrained\n",
            "    return tokenizer_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py\", line 2046, in from_pretrained\n",
            "    raise EnvironmentError(\n",
            "OSError: Can't load tokenizer for 'Trelis/mamba-2.8b-slimpj-bf16'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'Trelis/mamba-2.8b-slimpj-bf16' is the correct path to a directory containing all relevant files for a GPTNeoXTokenizerFast tokenizer.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Explicit Example"
      ],
      "metadata": {
        "id": "DLAR5OJV5z_2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from mamba_ssm.models.mixer_seq_simple import MambaLMHeadModel"
      ],
      "metadata": {
        "id": "8JhLWgBc51f7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set parameters\n",
        "model_name = \"Trelis/mamba-2.8b-slimpj-bf16\" # or \"clibrain/mamba-2.8b-instruct-openhermes\"\n",
        "# model_name = \"clibrain/mamba-2.8b-instruct-openhermes\"\n",
        "promptlen = 100\n",
        "genlen = 100\n",
        "temperature = 0.01\n",
        "topk = 1\n",
        "topp = 1.0\n",
        "repetition_penalty = 1.0\n",
        "batch = 1\n",
        "repeats = 3\n",
        "device = \"cuda\"\n",
        "dtype = torch.float16 #or use torch.bfloat16 for an A100 GPU\n",
        "\n",
        "# Load model\n",
        "print(f\"Loading model {model_name}\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-neox-20b\")\n",
        "model = MambaLMHeadModel.from_pretrained(\n",
        "    model_name,\n",
        "    device=device,\n",
        "    dtype=dtype)\n",
        "\n",
        "model.eval()\n",
        "print(f\"Number of parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad)}\")"
      ],
      "metadata": {
        "id": "BzpPbmwR6Hhe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(model.config)"
      ],
      "metadata": {
        "id": "SixQ_-kxXPJn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(model)"
      ],
      "metadata": {
        "id": "6PUdxkBaXo4w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt = \"Question: Write a short python program that adds together the first five fib numbers.\\n\\nAnswer: Here is the answer:\\n\"  # Replace with your prompt if needed\n",
        "prompt = \"What planets are in our solar system?\\n\\nAnswer: Here is the answer:\\n\"  # Replace with your prompt if needed\n",
        "\n",
        "torch.random.manual_seed(0)\n",
        "if prompt is None:\n",
        "    input_ids = torch.randint(1, 1000, (batch, promptlen), dtype=torch.long, device=device)\n",
        "    attn_mask = torch.ones_like(input_ids, dtype=torch.long, device=device)\n",
        "else:\n",
        "    tokens = tokenizer(prompt, return_tensors=\"pt\")\n",
        "    input_ids = tokens.input_ids.to(device=device)\n",
        "    attn_mask = tokens.attention_mask.to(device=device)\n",
        "max_length = input_ids.shape[1] + genlen\n",
        "\n",
        "fn = lambda: model.generate(\n",
        "    input_ids=input_ids,\n",
        "    max_length=max_length,\n",
        "    cg=True,\n",
        "    return_dict_in_generate=True,\n",
        "    output_scores=True,\n",
        "    enable_timing=False,\n",
        "    temperature=temperature,\n",
        "    top_k=topk,\n",
        "    top_p=topp,\n",
        "    eos_token_id=tokenizer.eos_token_id,\n",
        "    repetition_penalty=repetition_penalty,\n",
        ")"
      ],
      "metadata": {
        "id": "Df9RlZDd6cZv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokenizer.eos_token_id)\n",
        "eos_token = tokenizer.convert_ids_to_tokens(tokenizer.eos_token_id)\n",
        "print(eos_token)"
      ],
      "metadata": {
        "id": "UNiQldcQIcrx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "out = fn()\n",
        "if prompt is not None:\n",
        "    print(prompt)\n",
        "    print(\"\\n\")  # Print new lines\n",
        "    # Decode and print the generated text\n",
        "    decoded_sequences = tokenizer.batch_decode(out.sequences.tolist())\n",
        "    for sequence in decoded_sequences:\n",
        "        generated_text = sequence[len(tokenizer.decode(input_ids[0], skip_special_tokens=True)):]\n",
        "        print(generated_text)\n",
        "\n",
        "torch.cuda.synchronize()\n",
        "start = time.time()\n",
        "for _ in range(repeats):\n",
        "    fn()\n",
        "torch.cuda.synchronize()\n",
        "print(f\"Prompt length: {len(input_ids[0])}, generation length: {len(out.sequences[0]) - len(input_ids[0])}\")\n",
        "print(f\"{model_name} prompt processing + decoding time: {(time.time() - start) / repeats * 1000:.0f}ms\")"
      ],
      "metadata": {
        "id": "-yTbli5aFrma"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fibonacci(n):\n",
        "    a, b = 0, 1\n",
        "    for i in range(n):\n",
        "        print(a)\n",
        "        a, b = b, a + b\n",
        "\n",
        "print(fibonacci(5))"
      ],
      "metadata": {
        "id": "a4YutqwoTrDa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Passkey Retrieval (you must first have loaded the model above in the explicit example)"
      ],
      "metadata": {
        "id": "Tf0gMIs6KZ_C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the passkey and text file\n",
        "passkey = \"(the passkey is 'u89dsnakj8')\"\n",
        "text_file = '../berkshire23.txt'\n",
        "\n",
        "# Define the length limit for the text\n",
        "len_limit = int(16000 * 0.8)  # 16k characters is about 4k tokens of context.\n",
        "\n",
        "# Calculate the position to insert the passkey\n",
        "n = int(len_limit / 5 * 0.5)  # Placement of the passkey as a % of total text.\n",
        "\n",
        "# Read the text from the file\n",
        "with open(text_file, 'r') as file:\n",
        "    text = file.read()\n",
        "\n",
        "# Split the text into words\n",
        "words = text.split()\n",
        "\n",
        "# Insert the passkey after the nth word\n",
        "words.insert(n, passkey)\n",
        "\n",
        "# Join back into a string and truncate to 'len_limit' characters\n",
        "modified_text = ' '.join(words)[:len_limit]\n",
        "\n",
        "# Define the prompt for the model\n",
        "prompt = f'Respond with the passkey contained within the below text.\\n\\n{modified_text}\\n\\nRespond with the passkey contained within the above text.\\n\\nThe passkey is:'\n",
        "\n",
        "# Prepare the input for the model\n",
        "tokens = tokenizer(prompt, return_tensors=\"pt\")\n",
        "input_ids = tokens.input_ids.to(device=device)\n",
        "attn_mask = tokens.attention_mask.to(device=device)\n",
        "max_length = input_ids.shape[1] + 100  # Assuming you want to generate 100 tokens\n",
        "\n",
        "# Generate function for Mamba model\n",
        "def generate_mamba(input_ids, max_length):\n",
        "    return model.generate(\n",
        "        input_ids=input_ids,\n",
        "        max_length=max_length,\n",
        "        cg=True,\n",
        "        return_dict_in_generate=True,\n",
        "        output_scores=True,\n",
        "        enable_timing=False,\n",
        "        temperature=0.01,\n",
        "        top_k=1,\n",
        "        top_p=1.0,\n",
        "        eos_token_id=tokenizer.eos_token_id,\n",
        "        repetition_penalty=1.0,\n",
        "    )\n",
        "\n",
        "# Run the generation\n",
        "out = generate_mamba(input_ids, max_length)\n",
        "\n",
        "# Decode and print the generated text\n",
        "decoded_sequences = tokenizer.batch_decode(out.sequences.tolist())\n",
        "for sequence in decoded_sequences:\n",
        "    generated_text = sequence[len(tokenizer.decode(input_ids[0], skip_special_tokens=True)):]\n",
        "    print(generated_text)\n",
        "\n",
        "# Optionally, measure the time taken for generation\n",
        "torch.cuda.synchronize()\n",
        "start = time.time()\n",
        "for _ in range(3):  # Number of repeats\n",
        "    generate_mamba(input_ids, max_length)\n",
        "torch.cuda.synchronize()\n",
        "end = time.time()\n",
        "print(f\"{model_name} prompt processing + decoding time: {(end - start) / 3 * 1000:.0f}ms\")"
      ],
      "metadata": {
        "id": "UcoyTxtDKbzQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## OpenHermes Inference"
      ],
      "metadata": {
        "id": "EOUwK_aWJf1F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from mamba_ssm.models.mixer_seq_simple import MambaLMHeadModel\n",
        "\n",
        "CHAT_TEMPLATE_ID = \"HuggingFaceH4/zephyr-7b-beta\"\n",
        "\n",
        "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
        "model_name = \"clibrain/mamba-2.8b-instruct-openhermes\"\n",
        "\n",
        "eos_token = \"<|endoftext|>\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "tokenizer.eos_token = eos_token\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.chat_template = AutoTokenizer.from_pretrained(CHAT_TEMPLATE_ID).chat_template\n",
        "\n",
        "# Uncomment the below if you haven't loaded a model above.\n",
        "# model = MambaLMHeadModel.from_pretrained(\n",
        "#         model_name, device=device, dtype=torch.float16)\n",
        "\n",
        "messages = []\n",
        "# prompt = \"Tell me 5 sites to visit in Spain\"\n",
        "# prompt = \"What are the planets in our solar system\"\n",
        "prompt = \"Write a piece of python code to add the first five fibonacci numbers\"\n",
        "messages.append(dict(role=\"user\", content=prompt))\n",
        "\n",
        "input_ids = tokenizer.apply_chat_template(\n",
        "            messages, return_tensors=\"pt\", add_generation_prompt=True\n",
        ").to(device)\n",
        "\n",
        "out = model.generate(\n",
        "    input_ids=input_ids,\n",
        "    max_length=250,\n",
        "    temperature=0.9,\n",
        "    top_p=0.7,\n",
        "    eos_token_id=tokenizer.eos_token_id,\n",
        ")\n",
        "\n",
        "decoded = tokenizer.batch_decode(out)\n",
        "assistant_message = (\n",
        "    decoded[0].split(\"<|assistant|>\\n\")[-1].replace(eos_token, \"\")\n",
        ")\n",
        "\n",
        "print(assistant_message)"
      ],
      "metadata": {
        "id": "_Ym-4At1JhzE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fibonacci_sequence = [0, 1]\n",
        "for i in range(2, 6):\n",
        "    fibonacci_sequence.append(fibonacci_sequence[i-1] + fibonacci_sequence[i-2])\n",
        "print(fibonacci_sequence)"
      ],
      "metadata": {
        "id": "kBVbbZ0MyGYT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Uploading Models to Hub"
      ],
      "metadata": {
        "id": "8_zAIFN1AzvE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "new_repo_name=\"Trelis/mamba-2.8b-slimpj\"\n",
        "\n",
        "model.save_pretrained(new_repo_name)"
      ],
      "metadata": {
        "id": "w_ptts2G6dH-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install huggingface-hub"
      ],
      "metadata": {
        "id": "97ENDppp_7mK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()"
      ],
      "metadata": {
        "id": "PSYzwT2F_9nj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import HfApi, upload_folder\n",
        "import os\n",
        "\n",
        "# Set the repository name and local folder path\n",
        "repo_id = \"Trelis/mamba-2.8b-slimpj\"\n",
        "folder_path = \"./Trelis/mamba-2.8b-slimpj\"  # Adjust this path to your local folder\n",
        "\n",
        "# Optionally, set your Hugging Face authentication token\n",
        "# You can retrieve your token from your Hugging Face account (https://huggingface.co/settings/tokens)\n",
        "# If you don't set the token here, the function will use the token stored on your machine\n",
        "# token = None  # Replace with your token if needed\n",
        "\n",
        "# Upload the folder to the repository\n",
        "upload_url = upload_folder(\n",
        "    repo_id=repo_id,\n",
        "    folder_path=folder_path,\n",
        "    # token=token  # Include this if you have set a token above\n",
        ")\n",
        "\n",
        "print(f\"Folder uploaded successfully. You can view it at: {upload_url}\")"
      ],
      "metadata": {
        "id": "E6Av3LotAoV1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd ../../\n",
        "%ls -l -a"
      ],
      "metadata": {
        "id": "NZdQsDX3-eg7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "trcLDfsF_YFc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}