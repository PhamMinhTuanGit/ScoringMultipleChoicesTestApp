{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cnn\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "import utils\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = '/Users/phamminhtuan/Desktop/AIChallenge/IMG_1581_iter_0.jpg'\n",
    "label_path = '/Users/phamminhtuan/Desktop/AIChallenge/IMG_1581_iter_0.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, image_path, label_path):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            image_path (str): Path to the input image.\n",
    "            label_path (str): Path to the label file (YOLO format).\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.image_path = image_path\n",
    "        self.label_path = label_path\n",
    "        self.inputs, self.targets = [], []\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.ToTensor(),  # Convert image to tensor\n",
    "            transforms.Resize((28, 28))  # Resize image to 28x28\n",
    "        ])\n",
    "        image = cv2.imread(self.image_path)\n",
    "\n",
    "        # Read label and coordinates\n",
    "        labels, coords = utils.read_file_to_tensors(self.label_path)\n",
    "\n",
    "        # Process each coordinate (bounding box)\n",
    "        for i, coord in enumerate(coords):\n",
    "            square = utils.find_yolov8_square(image, coord)  # Get bounding box\n",
    "            cropped_image = utils.get_box(image, square)  # Crop the image\n",
    "            cropped_image = cv2.cvtColor(cropped_image, cv2.COLOR_BGR2RGB)  # Convert BGR to RGB\n",
    "            \n",
    "            # Apply transformation\n",
    "            cropped_tensor = self.transform(cropped_image)  # Convert to tensor & resize\n",
    "            \n",
    "            self.inputs.append(cropped_tensor)\n",
    "            self.targets.append(labels[i])  # Append corresponding label\n",
    "\n",
    "        # Convert inputs and targets to tensors\n",
    "        \n",
    "       \n",
    "    def __getitem__(self, idx):\n",
    "        image = self.inputs[idx]\n",
    "        label = self.targets[idx]\n",
    "        return image, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "image_path = '/Users/phamminhtuan/Desktop/AIChallenge/IMG_1581_iter_0.jpg'\n",
    "label_path = '/Users/phamminhtuan/Desktop/AIChallenge/IMG_1581_iter_0.txt'\n",
    "train_dataset = MyDataset(image_path, label_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [0.8104403..1.0000001].\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x311addf10>"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "image = train_dataset[0][0].permute(1,2,0)\n",
    "plt.imshow(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "571"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=16, shuffle= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        # Convolutional layers\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1)  # Input: 3x28x28 -> Output: 16x28x28\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1)  # Input: 16x28x28 -> Output: 32x28x28\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)  # Downsample by 2x\n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(32 * 14 * 14, 128)  # Flattened size after pooling\n",
    "        self.fc2 = nn.Linear(128, 2)  # 2 classes (e.g., 0: not filled, 1: filled)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))  # Conv1 + ReLU + MaxPooling\n",
    "        x = self.pool(F.relu(self.conv2(x)))  # Conv2 + ReLU + MaxPooling\n",
    "        x = torch.flatten(x, 1)  # Flatten for fully connected layers\n",
    "        x = F.relu(self.fc1(x))  # Fully connected layer 1 + ReLU\n",
    "        x = self.fc2(x)  # Fully connected layer 2 (no activation for logits)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"mps\")\n",
    "model = SimpleCNN()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "lr = 1e-3\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=lr)\n",
    "epochs = 5\n",
    "import time\n",
    "\n",
    "train_img = train_dataset[:][0]\n",
    "train_label = train_dataset[:][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (32x49 and 1568x128)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[105], line 18\u001b[0m\n\u001b[1;32m     15\u001b[0m target \u001b[38;5;241m=\u001b[39m train_label[i]\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m loss_batch \u001b[38;5;241m=\u001b[39m criterion(output, target)\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# Backward pass và cập nhật trọng số\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[83], line 16\u001b[0m, in \u001b[0;36mSimpleCNN.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     14\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpool(F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv2(x)))  \u001b[38;5;66;03m# Conv2 + ReLU + MaxPooling\u001b[39;00m\n\u001b[1;32m     15\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mflatten(x, \u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# Flatten for fully connected layers\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfc1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)  \u001b[38;5;66;03m# Fully connected layer 1 + ReLU\u001b[39;00m\n\u001b[1;32m     17\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc2(x)  \u001b[38;5;66;03m# Fully connected layer 2 (no activation for logits)\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/nn/modules/linear.py:117\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (32x49 and 1568x128)"
     ]
    }
   ],
   "source": [
    "full_loss = []\n",
    "full_error = []\n",
    "start = time.time()\n",
    "num_batches = 0\n",
    "for epoch in range(epochs):\n",
    "    model.train()  # Chuyển mô hình sang chế độ huấn luyện\n",
    "    running_loss = 0\n",
    "    running_error = 0\n",
    "    if epoch % 5 == 0:\n",
    "      lr = lr / 2\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "    for i in range(0, 571):  # Chia tập dữ liệu thành các batch size = 64\n",
    "        optimizer.zero_grad()  # Reset gradient\n",
    "        image = train_img[i]\n",
    "        target = train_label[i]\n",
    "\n",
    "        # Forward pass\n",
    "        output = model(image)\n",
    "        loss_batch = criterion(output, target)\n",
    "\n",
    "        # Backward pass và cập nhật trọng số\n",
    "        loss_batch.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Tính tổng loss và error cho toàn bộ epoch\n",
    "        running_loss += loss_batch.item()\n",
    "        _, pred = torch.max(output.data, 1)\n",
    "        running_error += (pred != target).sum().item()\n",
    "        num_batches += 1\n",
    "    # Tính giá trị trung bình loss và error\n",
    "    avg_loss = running_loss / num_batches   # 500 batches\n",
    "    avg_error = running_error / 794  # Tổng số mẫu là 50000\n",
    "\n",
    "    # Lưu lại giá trị loss và error\n",
    "    full_loss.append(avg_loss)\n",
    "    full_error.append(avg_error)\n",
    "\n",
    "    # In thông tin về epoch hiện tại\n",
    "    print(f'Epoch: {epoch+1}/{epochs}, Loss: {avg_loss:.4f}, Error: {avg_error*100:.2f}%')\n",
    "\n",
    "    # Bạn có thể thêm eval trên tập kiểm tra ở đây nếu muốn\n",
    "\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
