{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import cv2\n",
    "import utils\n",
    "import matplotlib.pyplot as plt\n",
    "inputs = []\n",
    "targets = []\n",
    "image = cv2.imread(\"/Users/phamminhtuan/Downloads/Trainning_SET/Images/IMG_1590_iter_0.jpg\")\n",
    "labels, coords = utils.read_file_to_tensors(\"/Users/phamminhtuan/Downloads/Trainning_SET/Labels/IMG_1590_iter_0.txt\")\n",
    "for i, coord in enumerate(coords):\n",
    "    x1, y1, x2, y2 = utils.find_yolov8_square(image, coord)\n",
    "    cv2.rectangle(image, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "cv2.imwrite(\"/Users/phamminhtuan/Desktop/AIChallenge/avg_coords.jpg\", image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import transforms\n",
    "class MyDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "    Initializes the dataset by reading the image and label files, and processes them into tensors.\n",
    "\n",
    "    Args:\n",
    "        image_path (str): Path to the input image.\n",
    "        label_path (str): Path to the label file (YOLO format).\n",
    "\n",
    "    The method reads the image from the specified path and the label data, then processes each labeled \n",
    "    bounding box to crop and transform the image into a tensor. The processed tensors and corresponding \n",
    "    labels are stored in `self.inputs` and `self.targets` respectively.\n",
    "    \"\"\"\n",
    "    def __init__(self, image_path, label_path):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.image_path = image_path\n",
    "        self.label_path = label_path\n",
    "        self.inputs, self.targets = [], []\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Resize((32, 32))\n",
    "        ])\n",
    "        image = cv2.imread(self.image_path)\n",
    "        labels, coords = utils.read_file_to_tensors(self.label_path)\n",
    "        for i, coord in enumerate(coords):\n",
    "            square = utils.find_yolov8_square(image, coord)\n",
    "            cropped_image = utils.get_box(image, square)\n",
    "            cropped_image = cv2.cvtColor(cropped_image, cv2.COLOR_BGR2RGB)\n",
    "            cropped_tensor = self.transform(cropped_image)\n",
    "            self.inputs.append(cropped_tensor)\n",
    "            self.targets.append(labels[i])\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.inputs[idx]\n",
    "        label = self.targets[idx]\n",
    "        return image, label\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "image_folder = \"/Users/phamminhtuan/Downloads/Trainning_SET/Images\"\n",
    "label_folder = \"/Users/phamminhtuan/Downloads/Trainning_SET/Labels\"\n",
    "train = []\n",
    "for filename in os.listdir(image_folder):\n",
    "    if filename.endswith('iter_0.jpg'):  # Lọc các file hình ảnh\n",
    "        image_path = os.path.join(image_folder, filename)\n",
    "        label_path = os.path.join(label_folder, os.path.splitext(filename)[0] + \".txt\")  # Giả định file nhãn cùng tên với file ảnh\n",
    "        temp = MyDataset(image_path,label_path)\n",
    "        for i in range(len(temp)):\n",
    "            train.append(temp[i])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "image_folder = \"/Users/phamminhtuan/Downloads/Trainning_SET/Images\"\n",
    "label_folder = \"/Users/phamminhtuan/Downloads/Trainning_SET/Labels\"\n",
    "test = []\n",
    "for filename in os.listdir(image_folder):\n",
    "    if filename.endswith('iter_1.jpg'):  # Lọc các file hình ảnh\n",
    "        image_path = os.path.join(image_folder, filename)\n",
    "        label_path = os.path.join(label_folder, os.path.splitext(filename)[0] + \".txt\")  # Giả định file nhãn cùng tên với file ảnh\n",
    "        temp_test = MyDataset(image_path,label_path)\n",
    "        for i in range(len(temp_test)):\n",
    "            test.append(temp_test[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "image_folder = \"/Users/phamminhtuan/Downloads/Trainning_SET/Images\"\n",
    "label_folder = \"/Users/phamminhtuan/Downloads/Trainning_SET/Labels\"\n",
    "test2 = []\n",
    "for filename in os.listdir(image_folder):\n",
    "    if filename.endswith('iter_2.jpg'):  # Lọc các file hình ảnh\n",
    "        image_path = os.path.join(image_folder, filename)\n",
    "        label_path = os.path.join(label_folder, os.path.splitext(filename)[0] + \".txt\")  # Giả định file nhãn cùng tên với file ảnh\n",
    "        temp_test = MyDataset(image_path,label_path)\n",
    "        for i in range(len(temp_test)):\n",
    "            test2.append(temp_test[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9283 9919 9920\n",
      "torch.Size([3, 32, 32]) torch.Size([])\n"
     ]
    }
   ],
   "source": [
    "print(len(train), len(test), len(test2))\n",
    "print(temp[0][0].shape,temp[0][1].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "291 310\n"
     ]
    }
   ],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train, batch_size=32, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(test2, batch_size=32, shuffle=True)\n",
    "print (len(train_loader), len(val_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "class EfficientCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initializes the EfficientCNN model. This model is an efficient convolutional neural network for the \n",
    "        classification task. It consists of a feature extraction part and a classification part. The feature \n",
    "        extraction part is a convolutional neural network which consists of 3 convolutional layers with \n",
    "        maxpooling, where the number of channels are 32, 64, 128 respectively. The classification part is a \n",
    "        fully connected neural network which consists of 2 fully connected layers with dropout rate 0.5.\n",
    "\n",
    "        Args:\n",
    "            None\n",
    "        \"\"\"\n",
    "        super(EfficientCNN, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(128 * 4 * 4, 256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(256, 2)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"mps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "def train_model(train_loader, val_loader, epochs=50):\n",
    "    \"\"\"\n",
    "    Train the EfficientCNN model on the given training data.\n",
    "\n",
    "    Args:\n",
    "        train_loader (DataLoader): A DataLoader for the training data.\n",
    "        val_loader (DataLoader): A DataLoader for the validation data.\n",
    "        epochs (int, optional): The number of epochs to train. Defaults to 50.\n",
    "        batch_size (int, optional): The batch size to use for training. Defaults to 64.\n",
    "\n",
    "    Returns:\n",
    "        nn.Module: The trained EfficientCNN model.\n",
    "    \"\"\"\n",
    "    #model = EfficientCNN().to(device)\n",
    "    model = EfficientCNN()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)\n",
    "    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=5)\n",
    "    best_val_accuracy = 0\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for inputs, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            labels = labels.long()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "        \n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                outputs = model(inputs)\n",
    "                labels = labels.long()\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        val_accuracy = 100 * correct / total\n",
    "        scheduler.step(val_loss)\n",
    "        print(f'Epoch {epoch+1}/{epochs}')\n",
    "        print(f'Train Loss: {train_loss/len(train_loader):.4f}')\n",
    "        print(f'Val Loss: {val_loss/len(val_loader):.4f}')\n",
    "        print(f'Val Accuracy: {val_accuracy:.2f}%')\n",
    "        \n",
    "        if val_accuracy > best_val_accuracy:\n",
    "            best_val_accuracy = val_accuracy\n",
    "            torch.save(model.state_dict(), '/Users/phamminhtuan/Desktop/AIChallenge/cnn.pth')\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "Train Loss: 0.0541\n",
      "Val Loss: 0.0234\n",
      "Val Accuracy: 99.31%\n",
      "Epoch 2/50\n",
      "Train Loss: 0.0233\n",
      "Val Loss: 0.0354\n",
      "Val Accuracy: 98.40%\n",
      "Epoch 3/50\n",
      "Train Loss: 0.0206\n",
      "Val Loss: 0.0133\n",
      "Val Accuracy: 99.67%\n",
      "Epoch 4/50\n",
      "Train Loss: 0.0172\n",
      "Val Loss: 0.0133\n",
      "Val Accuracy: 99.60%\n",
      "Epoch 5/50\n",
      "Train Loss: 0.0140\n",
      "Val Loss: 0.0193\n",
      "Val Accuracy: 99.58%\n",
      "Epoch 6/50\n",
      "Train Loss: 0.0129\n",
      "Val Loss: 0.0671\n",
      "Val Accuracy: 99.06%\n",
      "Epoch 7/50\n",
      "Train Loss: 0.0123\n",
      "Val Loss: 0.0091\n",
      "Val Accuracy: 99.72%\n",
      "Epoch 8/50\n",
      "Train Loss: 0.0099\n",
      "Val Loss: 0.0203\n",
      "Val Accuracy: 99.39%\n",
      "Epoch 9/50\n",
      "Train Loss: 0.0117\n",
      "Val Loss: 0.2960\n",
      "Val Accuracy: 88.26%\n",
      "Epoch 10/50\n",
      "Train Loss: 0.0106\n",
      "Val Loss: 0.0230\n",
      "Val Accuracy: 99.49%\n",
      "Epoch 11/50\n",
      "Train Loss: 0.0078\n",
      "Val Loss: 0.0076\n",
      "Val Accuracy: 99.80%\n",
      "Epoch 12/50\n",
      "Train Loss: 0.0078\n",
      "Val Loss: 0.0086\n",
      "Val Accuracy: 99.82%\n",
      "Epoch 13/50\n",
      "Train Loss: 0.0064\n",
      "Val Loss: 0.0221\n",
      "Val Accuracy: 99.61%\n",
      "Epoch 14/50\n",
      "Train Loss: 0.0088\n",
      "Val Loss: 0.0116\n",
      "Val Accuracy: 99.78%\n",
      "Epoch 15/50\n",
      "Train Loss: 0.0085\n",
      "Val Loss: 0.0109\n",
      "Val Accuracy: 99.81%\n",
      "Epoch 16/50\n",
      "Train Loss: 0.0084\n",
      "Val Loss: 0.0184\n",
      "Val Accuracy: 99.53%\n",
      "Epoch 17/50\n",
      "Train Loss: 0.0052\n",
      "Val Loss: 0.0092\n",
      "Val Accuracy: 99.74%\n",
      "Epoch 18/50\n",
      "Train Loss: 0.0031\n",
      "Val Loss: 0.0081\n",
      "Val Accuracy: 99.79%\n",
      "Epoch 19/50\n",
      "Train Loss: 0.0023\n",
      "Val Loss: 0.0078\n",
      "Val Accuracy: 99.84%\n",
      "Epoch 20/50\n",
      "Train Loss: 0.0015\n",
      "Val Loss: 0.0094\n",
      "Val Accuracy: 99.79%\n",
      "Epoch 21/50\n",
      "Train Loss: 0.0017\n",
      "Val Loss: 0.0081\n",
      "Val Accuracy: 99.82%\n",
      "Epoch 22/50\n",
      "Train Loss: 0.0013\n",
      "Val Loss: 0.0076\n",
      "Val Accuracy: 99.85%\n",
      "Epoch 23/50\n",
      "Train Loss: 0.0012\n",
      "Val Loss: 0.0074\n",
      "Val Accuracy: 99.85%\n",
      "Epoch 24/50\n",
      "Train Loss: 0.0008\n",
      "Val Loss: 0.0078\n",
      "Val Accuracy: 99.81%\n",
      "Epoch 25/50\n",
      "Train Loss: 0.0008\n",
      "Val Loss: 0.0085\n",
      "Val Accuracy: 99.80%\n",
      "Epoch 26/50\n",
      "Train Loss: 0.0006\n",
      "Val Loss: 0.0081\n",
      "Val Accuracy: 99.84%\n",
      "Epoch 27/50\n",
      "Train Loss: 0.0004\n",
      "Val Loss: 0.0082\n",
      "Val Accuracy: 99.86%\n",
      "Epoch 28/50\n",
      "Train Loss: 0.0005\n",
      "Val Loss: 0.0091\n",
      "Val Accuracy: 99.82%\n",
      "Epoch 29/50\n",
      "Train Loss: 0.0012\n",
      "Val Loss: 0.0086\n",
      "Val Accuracy: 99.82%\n",
      "Epoch 30/50\n",
      "Train Loss: 0.0008\n",
      "Val Loss: 0.0079\n",
      "Val Accuracy: 99.84%\n",
      "Epoch 31/50\n",
      "Train Loss: 0.0005\n",
      "Val Loss: 0.0077\n",
      "Val Accuracy: 99.85%\n",
      "Epoch 32/50\n",
      "Train Loss: 0.0005\n",
      "Val Loss: 0.0089\n",
      "Val Accuracy: 99.81%\n",
      "Epoch 33/50\n",
      "Train Loss: 0.0005\n",
      "Val Loss: 0.0078\n",
      "Val Accuracy: 99.85%\n",
      "Epoch 34/50\n",
      "Train Loss: 0.0005\n",
      "Val Loss: 0.0078\n",
      "Val Accuracy: 99.85%\n",
      "Epoch 35/50\n",
      "Train Loss: 0.0004\n",
      "Val Loss: 0.0080\n",
      "Val Accuracy: 99.85%\n",
      "Epoch 36/50\n",
      "Train Loss: 0.0004\n",
      "Val Loss: 0.0088\n",
      "Val Accuracy: 99.82%\n",
      "Epoch 37/50\n",
      "Train Loss: 0.0004\n",
      "Val Loss: 0.0084\n",
      "Val Accuracy: 99.84%\n",
      "Epoch 38/50\n",
      "Train Loss: 0.0003\n",
      "Val Loss: 0.0082\n",
      "Val Accuracy: 99.84%\n",
      "Epoch 39/50\n",
      "Train Loss: 0.0002\n",
      "Val Loss: 0.0094\n",
      "Val Accuracy: 99.81%\n",
      "Epoch 40/50\n",
      "Train Loss: 0.0003\n",
      "Val Loss: 0.0083\n",
      "Val Accuracy: 99.84%\n",
      "Epoch 41/50\n",
      "Train Loss: 0.0004\n",
      "Val Loss: 0.0083\n",
      "Val Accuracy: 99.85%\n",
      "Epoch 42/50\n",
      "Train Loss: 0.0004\n",
      "Val Loss: 0.0083\n",
      "Val Accuracy: 99.83%\n",
      "Epoch 43/50\n",
      "Train Loss: 0.0006\n",
      "Val Loss: 0.0086\n",
      "Val Accuracy: 99.82%\n",
      "Epoch 44/50\n",
      "Train Loss: 0.0005\n",
      "Val Loss: 0.0086\n",
      "Val Accuracy: 99.82%\n",
      "Epoch 45/50\n",
      "Train Loss: 0.0004\n",
      "Val Loss: 0.0080\n",
      "Val Accuracy: 99.85%\n",
      "Epoch 46/50\n",
      "Train Loss: 0.0003\n",
      "Val Loss: 0.0082\n",
      "Val Accuracy: 99.85%\n",
      "Epoch 47/50\n",
      "Train Loss: 0.0004\n",
      "Val Loss: 0.0087\n",
      "Val Accuracy: 99.81%\n",
      "Epoch 48/50\n",
      "Train Loss: 0.0003\n",
      "Val Loss: 0.0085\n",
      "Val Accuracy: 99.83%\n",
      "Epoch 49/50\n",
      "Train Loss: 0.0004\n",
      "Val Loss: 0.0082\n",
      "Val Accuracy: 99.83%\n",
      "Epoch 50/50\n",
      "Train Loss: 0.0004\n",
      "Val Loss: 0.0083\n",
      "Val Accuracy: 99.85%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "EfficientCNN(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (4): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (5): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (6): ReLU(inplace=True)\n",
       "    (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (8): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (9): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (10): ReLU(inplace=True)\n",
       "    (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=2048, out_features=256, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Dropout(p=0.5, inplace=False)\n",
       "    (3): Linear(in_features=256, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_model(train_loader, val_loader, epochs=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data augment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_duplicate_lines(file_path):\n",
    "    try:\n",
    "        # Đọc nội dung file và loại bỏ các dòng trùng lặp\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            lines = file.readlines()\n",
    "            unique_lines = list(dict.fromkeys(lines))  # Loại bỏ các dòng trùng lặp và giữ nguyên thứ tự\n",
    "        \n",
    "        # Ghi lại nội dung không trùng lặp vào file\n",
    "        with open(file_path, 'w', encoding='utf-8') as file:\n",
    "            file.writelines(unique_lines)\n",
    "        print(\"Đã loại bỏ các dòng trùng lặp thành công.\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File {file_path} không tồn tại.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Đã xảy ra lỗi: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_duplicates_column2_and_column3(file_path, output_path):\n",
    "    try:\n",
    "        # Đọc file và lưu dữ liệu\n",
    "        with open(file_path, 'r', encoding='utf-8') as infile:\n",
    "            lines = infile.readlines()\n",
    "\n",
    "        # Tách các dòng thành danh sách các cột\n",
    "        rows = [line.strip().split() for line in lines]\n",
    "\n",
    "        # Lưu tần suất xuất hiện của các giá trị ở cột 2 và cột 3\n",
    "        counts = {}\n",
    "        for row in rows:\n",
    "            if len(row) > 2:  # Đảm bảo có ít nhất 3 cột\n",
    "                key = (row[1], row[2])  # Chỉ lấy giá trị ở cột 2 và cột 3\n",
    "                counts[key] = counts.get(key, 0) + 1\n",
    "\n",
    "        # Lọc bỏ các dòng có giá trị ở cột 2 và cột 3 trùng lặp\n",
    "        cleaned_rows = [\n",
    "            row for row in rows\n",
    "            if len(row) <= 2 or counts.get((row[1], row[2]), 0) == 1\n",
    "        ]\n",
    "\n",
    "        # Ghi lại các dòng không trùng lặp vào file đầu ra\n",
    "        with open(output_path, 'w', encoding='utf-8') as outfile:\n",
    "            for row in cleaned_rows:\n",
    "                outfile.write(' '.join(row) + '\\n')  # Sử dụng khoảng trắng để ngăn cách các cột\n",
    "\n",
    "        print(\"Đã loại bỏ các dòng trùng lặp ở cột 2 và cột 3.\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File {file_path} không tồn tại.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Đã xảy ra lỗi: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [0.7982613..1.0000001].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "i = int(torch.rand(1) * 17000)\n",
    "plt.imshow(train_img[i].permute(1,2,0))\n",
    "print(train_label[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Đã loại bỏ các dòng trùng lặp thành công.\n",
      "Đã loại bỏ các dòng trùng lặp ở cột 2 và cột 3.\n",
      "/Users/phamminhtuan/Desktop/Trainning_SET/Images/IMG_2269_iter_2.jpg\n",
      "/Users/phamminhtuan/Downloads/Trainning_SET/Labels/IMG_2269_iter_2.txt\n",
      "418\n",
      "Đã loại bỏ các dòng trùng lặp thành công.\n",
      "Đã loại bỏ các dòng trùng lặp ở cột 2 và cột 3.\n",
      "/Users/phamminhtuan/Desktop/Trainning_SET/Images/IMG_1587_iter_2.jpg\n",
      "/Users/phamminhtuan/Downloads/Trainning_SET/Labels/IMG_1587_iter_2.txt\n",
      "572\n",
      "Đã loại bỏ các dòng trùng lặp thành công.\n",
      "Đã loại bỏ các dòng trùng lặp ở cột 2 và cột 3.\n",
      "/Users/phamminhtuan/Desktop/Trainning_SET/Images/IMG_2270_iter_2.jpg\n",
      "/Users/phamminhtuan/Downloads/Trainning_SET/Labels/IMG_2270_iter_2.txt\n",
      "425\n",
      "Đã loại bỏ các dòng trùng lặp thành công.\n",
      "Đã loại bỏ các dòng trùng lặp ở cột 2 và cột 3.\n",
      "/Users/phamminhtuan/Desktop/Trainning_SET/Images/IMG_1591_iter_2.jpg\n",
      "/Users/phamminhtuan/Downloads/Trainning_SET/Labels/IMG_1591_iter_2.txt\n",
      "572\n",
      "Đã loại bỏ các dòng trùng lặp thành công.\n",
      "Đã loại bỏ các dòng trùng lặp ở cột 2 và cột 3.\n",
      "/Users/phamminhtuan/Desktop/Trainning_SET/Images/IMG_2266_iter_2.jpg\n",
      "/Users/phamminhtuan/Downloads/Trainning_SET/Labels/IMG_2266_iter_2.txt\n",
      "416\n",
      "Đã loại bỏ các dòng trùng lặp thành công.\n",
      "Đã loại bỏ các dòng trùng lặp ở cột 2 và cột 3.\n",
      "/Users/phamminhtuan/Desktop/Trainning_SET/Images/IMG_3958_iter_2.jpg\n",
      "/Users/phamminhtuan/Downloads/Trainning_SET/Labels/IMG_3958_iter_2.txt\n",
      "572\n",
      "Đã loại bỏ các dòng trùng lặp thành công.\n",
      "Đã loại bỏ các dòng trùng lặp ở cột 2 và cột 3.\n",
      "/Users/phamminhtuan/Desktop/Trainning_SET/Images/IMG_3955_iter_2.jpg\n",
      "/Users/phamminhtuan/Downloads/Trainning_SET/Labels/IMG_3955_iter_2.txt\n",
      "572\n",
      "Đã loại bỏ các dòng trùng lặp thành công.\n",
      "Đã loại bỏ các dòng trùng lặp ở cột 2 và cột 3.\n",
      "/Users/phamminhtuan/Desktop/Trainning_SET/Images/IMG_2272_iter_2.jpg\n",
      "/Users/phamminhtuan/Downloads/Trainning_SET/Labels/IMG_2272_iter_2.txt\n",
      "414\n",
      "Đã loại bỏ các dòng trùng lặp thành công.\n",
      "Đã loại bỏ các dòng trùng lặp ở cột 2 và cột 3.\n",
      "/Users/phamminhtuan/Desktop/Trainning_SET/Images/IMG_1586_iter_2.jpg\n",
      "/Users/phamminhtuan/Downloads/Trainning_SET/Labels/IMG_1586_iter_2.txt\n",
      "572\n",
      "Đã loại bỏ các dòng trùng lặp thành công.\n",
      "Đã loại bỏ các dòng trùng lặp ở cột 2 và cột 3.\n",
      "/Users/phamminhtuan/Desktop/Trainning_SET/Images/IMG_2271_iter_2.jpg\n",
      "/Users/phamminhtuan/Downloads/Trainning_SET/Labels/IMG_2271_iter_2.txt\n",
      "425\n",
      "Đã loại bỏ các dòng trùng lặp thành công.\n",
      "Đã loại bỏ các dòng trùng lặp ở cột 2 và cột 3.\n",
      "/Users/phamminhtuan/Desktop/Trainning_SET/Images/IMG_2274_iter_2.jpg\n",
      "/Users/phamminhtuan/Downloads/Trainning_SET/Labels/IMG_2274_iter_2.txt\n",
      "421\n",
      "Đã loại bỏ các dòng trùng lặp thành công.\n",
      "Đã loại bỏ các dòng trùng lặp ở cột 2 và cột 3.\n",
      "/Users/phamminhtuan/Desktop/Trainning_SET/Images/IMG_1592_iter_2.jpg\n",
      "/Users/phamminhtuan/Downloads/Trainning_SET/Labels/IMG_1592_iter_2.txt\n",
      "572\n",
      "Đã loại bỏ các dòng trùng lặp thành công.\n",
      "Đã loại bỏ các dòng trùng lặp ở cột 2 và cột 3.\n",
      "/Users/phamminhtuan/Desktop/Trainning_SET/Images/IMG_2268_iter_2.jpg\n",
      "/Users/phamminhtuan/Downloads/Trainning_SET/Labels/IMG_2268_iter_2.txt\n",
      "419\n",
      "Đã loại bỏ các dòng trùng lặp thành công.\n",
      "Đã loại bỏ các dòng trùng lặp ở cột 2 và cột 3.\n",
      "/Users/phamminhtuan/Desktop/Trainning_SET/Images/IMG_2265_iter_2.jpg\n",
      "/Users/phamminhtuan/Downloads/Trainning_SET/Labels/IMG_2265_iter_2.txt\n",
      "421\n",
      "Đã loại bỏ các dòng trùng lặp thành công.\n",
      "Đã loại bỏ các dòng trùng lặp ở cột 2 và cột 3.\n",
      "/Users/phamminhtuan/Desktop/Trainning_SET/Images/IMG_1584_iter_2.jpg\n",
      "/Users/phamminhtuan/Downloads/Trainning_SET/Labels/IMG_1584_iter_2.txt\n",
      "572\n",
      "Đã loại bỏ các dòng trùng lặp thành công.\n",
      "Đã loại bỏ các dòng trùng lặp ở cột 2 và cột 3.\n",
      "/Users/phamminhtuan/Desktop/Trainning_SET/Images/IMG_1589_iter_2.jpg\n",
      "/Users/phamminhtuan/Downloads/Trainning_SET/Labels/IMG_1589_iter_2.txt\n",
      "572\n",
      "Đã loại bỏ các dòng trùng lặp thành công.\n",
      "Đã loại bỏ các dòng trùng lặp ở cột 2 và cột 3.\n",
      "/Users/phamminhtuan/Desktop/Trainning_SET/Images/IMG_2273_iter_2.jpg\n",
      "/Users/phamminhtuan/Downloads/Trainning_SET/Labels/IMG_2273_iter_2.txt\n",
      "429\n",
      "Đã loại bỏ các dòng trùng lặp thành công.\n",
      "Đã loại bỏ các dòng trùng lặp ở cột 2 và cột 3.\n",
      "/Users/phamminhtuan/Desktop/Trainning_SET/Images/IMG_1590_iter_2.jpg\n",
      "/Users/phamminhtuan/Downloads/Trainning_SET/Labels/IMG_1590_iter_2.txt\n",
      "573\n",
      "Đã loại bỏ các dòng trùng lặp thành công.\n",
      "Đã loại bỏ các dòng trùng lặp ở cột 2 và cột 3.\n",
      "/Users/phamminhtuan/Desktop/Trainning_SET/Images/IMG_2267_iter_2.jpg\n",
      "/Users/phamminhtuan/Downloads/Trainning_SET/Labels/IMG_2267_iter_2.txt\n",
      "412\n",
      "Đã loại bỏ các dòng trùng lặp thành công.\n",
      "Đã loại bỏ các dòng trùng lặp ở cột 2 và cột 3.\n",
      "/Users/phamminhtuan/Desktop/Trainning_SET/Images/IMG_1581_iter_2.jpg\n",
      "/Users/phamminhtuan/Downloads/Trainning_SET/Labels/IMG_1581_iter_2.txt\n",
      "571\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "image_folder = \"/Users/phamminhtuan/Desktop/Trainning_SET/Images\"\n",
    "label_folder = \"/Users/phamminhtuan/Downloads/Trainning_SET/Labels\"\n",
    "draw_label_folder = \"/Users/phamminhtuan/Downloads/Trainning_SET/drawLabels\"\n",
    "for filename in os.listdir(image_folder):\n",
    "    if filename.endswith('iter_2.jpg'):    \n",
    "        image_path = os.path.join(image_folder, filename)\n",
    "        label_path = os.path.join(label_folder, os.path.splitext(filename)[0] + \".txt\")\n",
    "        remove_duplicate_lines(label_path)\n",
    "        remove_duplicates_column2_and_column3(label_path, label_path)\n",
    "        temp = MyDataset(image_path=image_path,label_path=label_path)\n",
    "        print(image_path)\n",
    "        print(label_path)\n",
    "        print(len(temp)) \n",
    "        image = cv2.imread(image_path)\n",
    "        labels, coords = utils.read_file_to_tensors(label_path)\n",
    "        for i, coord in enumerate(coords):\n",
    "            x1, y1, x2, y2 = utils.find_yolov8_square(image, coord)\n",
    "            cv2.rectangle(image, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "        cv2.imwrite(os.path.join(draw_label_folder, os.path.splitext(filename)[0] + \".jpg\"), image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test lại"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "def test_model(model, val_loader, device='cpu'):\n",
    "    \"\"\"\n",
    "    Test the given model on the test dataset.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The trained model to evaluate.\n",
    "        test_loader (DataLoader): A DataLoader for the test data.\n",
    "        device (str, optional): The device to use for testing ('cpu' or 'cuda'). Defaults to 'cpu'.\n",
    "\n",
    "    Returns:\n",
    "        float: Test accuracy (in percentage).\n",
    "        float: Test loss.\n",
    "    \"\"\"\n",
    "    model = model.to(device)\n",
    "    model.eval()  # Chuyển sang chế độ đánh giá\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():  # Tắt tính toán gradient để tăng tốc\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            labels = labels.long()\n",
    "            loss = criterion(outputs, labels)\n",
    "            test_loss += loss.item()\n",
    "            \n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    test_accuracy = 100 * correct / total\n",
    "    average_test_loss = test_loss / len(val_loader)\n",
    "\n",
    "    print(f'Test Loss: {average_test_loss:.4f}')\n",
    "    print(f'Test Accuracy: {test_accuracy:.2f}%')\n",
    "\n",
    "    return test_accuracy, average_test_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/hv/wh1bzsb91mz5swh0znhrdrv00000gn/T/ipykernel_18657/1814793565.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('/Users/phamminhtuan/Desktop/AIChallenge/cnn.pth'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.0059\n",
      "Test Accuracy: 99.88%\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model = EfficientCNN()\n",
    "model.load_state_dict(torch.load('/Users/phamminhtuan/Desktop/AIChallenge/cnn.pth'))\n",
    "model = model.to(device)\n",
    "\n",
    "# Đánh giá mô hình\n",
    "test_accuracy, test_loss = test_model(model, val_loader, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [0.7021082..1.0000001].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-8.2792,  9.2511]], grad_fn=<AddmmBackward0>)\n",
      "tensor(1.)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x320a5a1b0>"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "output = model(image.unsqueeze(0))\n",
    "print(output)\n",
    "print(test[0][1])\n",
    "plt.imshow(image.permute(1,2,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = \"/Users/phamminhtuan/Desktop/Trainning_SET/Images/IMG_1587_iter_1.jpg\"\n",
    "label_path = \"/Users/phamminhtuan/Desktop/Trainning_SET/Labels/IMG_1587_iter_1.txt\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import utils\n",
    "import torchvision \n",
    "image = cv2.imread(image_path)\n",
    "labels, coords = utils.read_file_to_tensors(label_path)\n",
    "for i, coord in enumerate(coords):\n",
    "    xy_coord = utils.find_yolov8_square(image, coord)\n",
    "    x1, y1, x2, y2 = xy_coord\n",
    "    input = utils.get_box(image, xy_coord)\n",
    "    transform = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Resize((32, 32))\n",
    "        ])\n",
    "    input = transform(input)\n",
    "    output = model(input.unsqueeze(0))\n",
    "    _, predicted = torch.max(output.data, 1)\n",
    "    if (predicted == 1):\n",
    "        cv2.rectangle(image, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "cv2.imwrite(\"/Users/phamminhtuan/Desktop/AIChallenge/avg_coords.jpg\", image)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
